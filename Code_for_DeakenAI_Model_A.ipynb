{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AugSubt_Densnet201__batch8__(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fj2O3Au9xdS"
      },
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#  Copyright (c) 2021. Mohamed Reda Bouadjenek, Deakin University              +\n",
        "#           Email:  reda.bouadjenek@deakin.edu.au                              +\n",
        "#                                                                              +\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");             +\n",
        "#   you may not use this file except in compliance with the License.           +\n",
        "#    You may obtain a copy of the License at:                                  +\n",
        "#                                                                              +\n",
        "#                 http://www.apache.org/licenses/LICENSE-2.0                   +\n",
        "#                                                                              +\n",
        "#    Unless required by applicable law or agreed to in writing, software       +\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,         +\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  +\n",
        "#    See the License for the specific language governing permissions and       +\n",
        "#    limitations under the License.                                            +\n",
        "#                                                                              +\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljDdUUqvUEWt"
      },
      "source": [
        "**Notebook author:** [Mohamed Reda Bouadjenek](https://rbouadjenek.github.io/), Lecturer of Applied Artificial Intelligence, \n",
        "\n",
        "**Institution:** Deakin University, School of Information Technology, Faculty of Sci Eng & Built Env\n",
        "\n",
        "**Adress:** Locked Bag 20000, Geelong, VIC 3220\n",
        "\n",
        "**Phone:** +61 3 522 78380\n",
        "\n",
        "**Email:** reda.bouadjenek@deakin.edu.au\n",
        "\n",
        "<img style=\"float: left;\" src=\"https://github.com/rbouadjenek/deakin-simpsons-challenge2020/blob/main/images/deakin2.png?raw=1\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzuxD_rTUEWu"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Welcome to the Notebook for the Deakin Simpsons Challenge 2021!**\n",
        "\n",
        "![](https://github.com/rbouadjenek/deakin-simpsons-challenge2020/blob/main/images/Simpsons_cast.png?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "This Notebook allows you to build a classification model for The Deakin Simpsons challenge 2021.\n",
        "\n",
        "The **Deakin Simpsons challenge 2021** is a computer vision competition for which the goal is to recognize Simpsons characters individually in images using machine learning/deep learning. The challenge is designed to provide students with the opportunity to work as team members, to compete with each other, and to enhance the student learning experience by improving their AI modeling, problem-solving, and team-working skills.\n",
        " \n",
        "\n",
        "\n",
        "As participants, your goal is to build a machine learning/deep learning model to automatically recognize the following Simpsons characters:\n",
        "\n",
        " \n",
        "1. [Abraham grampa simpson](https://en.wikipedia.org/wiki/Grampa_Simpson)\n",
        "2. [Apu nahasapeemapetilon](https://en.wikipedia.org/wiki/Apu_Nahasapeemapetilon)\n",
        "3. [Bart simpson](https://en.wikipedia.org/wiki/Bart_Simpson)\n",
        "4. [Charles montgomery burns](https://en.wikipedia.org/wiki/Mr._Burns)\n",
        "5. [chief wiggum](https://en.wikipedia.org/wiki/Chief_Wiggum)\n",
        "6. [Comic book guy](https://en.wikipedia.org/wiki/Comic_Book_Guy)\n",
        "7. [Edna krabappel](https://en.wikipedia.org/wiki/Edna_Krabappel)\n",
        "8. [Homer simpson](https://en.wikipedia.org/wiki/Homer_Simpson)\n",
        "9. [Kent brockman](https://en.wikipedia.org/wiki/Kent_Brockman)\n",
        "10. [Krusty the clown](https://en.wikipedia.org/wiki/Krusty_the_Clown)\n",
        "11. [Lenny leonard](https://simpsons.fandom.com/wiki/Lenny_Leonard)\n",
        "12. [Lisa simpson](https://en.wikipedia.org/wiki/Lisa_Simpson)\n",
        "13. [Marge simpson](https://en.wikipedia.org/wiki/Marge_Simpson)\n",
        "14. [Mayor quimby](https://en.wikipedia.org/wiki/Mayor_Quimby)\n",
        "15. [Milhouse van houten](https://en.wikipedia.org/wiki/Milhouse_Van_Houten)\n",
        "16. [Moe szyslak](https://en.wikipedia.org/wiki/Moe_Szyslak)\n",
        "17. [Ned flanders](https://en.wikipedia.org/wiki/Ned_Flanders)\n",
        "18. [Nelson muntz](https://en.wikipedia.org/wiki/Nelson_Muntz)\n",
        "19. [Principal skinner](https://en.wikipedia.org/wiki/Principal_Skinner)\n",
        "20. [Sideshow bob](https://en.wikipedia.org/wiki/Sideshow_Bob)\n",
        "\n",
        "\n",
        "To achieve this taks, you will be given a data set that consists of 19,548 images to train your model and to tune your hyperparameters. However, feel free to extend it by collecting new images or by using data augmentation techniques.\n",
        "\n",
        "Once you have built your model, you will have to submit it on the [CodaLab](https://competitions.codalab.org/competitions/27191?secret_key=f0a7cc3e-7f78-4bb1-8564-95bc2fadafa5) platform to be evaluated. \n",
        "We evaluate the performance of your model using the [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)  on a private test set that we have directly collected and labeled from TV show episodes.\n",
        "Once the evaluation completed, your entry will appear on the leaderboard to see your performance against other competitors.\n",
        "\n",
        "\n",
        "In the following, we will take you through  a 6-step process to build a simple model to perform this task as follows:\n",
        "\n",
        "1. `Setup the environment:` Thie first step consists of setting the environement and downloading the data.\n",
        "2. `Preprocessing:` The second step is a preprocessing step that consists of resizing, plitting, and piping the input data.\n",
        "3. `Exploring the data:` The third step consists of a simple data exploration step where you will see samples of the data and some statistics to help you in understanding the data.\n",
        "4. `Designing the model:` The forth step consists of designing an architecture for the task.\n",
        "5. `Traning:` The fifth step consists of starting the training process.\n",
        "6. `Monitoring:` The sixth step consists of monitoring the traning process to investigate possible overfitting.\n",
        "7. `Submission:` The seventh and last step will take you through the submission process.\n",
        "\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [The Simpsons characters recognition and detection using Keras](https://medium.com/alex-attia-blog/the-simpsons-character-recognition-using-keras-d8e1796eae36)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWE9LfY-UEWv"
      },
      "source": [
        "# Setup the environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKHRAFJ-UEWv"
      },
      "source": [
        "First, it is important to mention that in order to submit you model to the leaderbord, you need to generate it and save it using  <span style=\"color:red;font-weight: bold;\">TensorFlow 2.2.0</span> and not  <span style=\"color:red;font-weight: bold;text-decoration: line-through;\">TensorFlow 2.3.0</span>. Therefore, please first run the following cell to install the appropriate <span style=\"color:red;font-weight: bold;\">TensorFlow version (2.2.0)</span>. You may need to restart your kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RlT-OG0vGdN"
      },
      "source": [
        "**The following code section is from the template provided by the AI Challenge organiser.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fusNkGWA9qF4",
        "outputId": "099c20c5-690f-40e6-f91b-a3a2251c492a"
      },
      "source": [
        "# Run this to install the appropriate tensorflow package\n",
        "!pip install tensorflow==2.2.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.36.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.30.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBqpEVhXrRuf"
      },
      "source": [
        "**The following code section is from the template provided by the AI Challenge organiser.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeYSYi7yUEWw"
      },
      "source": [
        "Once the appropriate TensorFlow version installed, you need now to load all the required packages for this Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbbWh-959k8g",
        "outputId": "4e4e45b2-82fe-4976-9bb1-5188fee3a5f5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score, classification_report\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.python.keras.saving import hdf5_format\n",
        "from keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
        "import h5py, itertools, collections\n",
        "import itertools\n",
        "\n",
        "##################\n",
        "# Verifications:\n",
        "#################\n",
        "print('GPU is used.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'GPU is NOT used.')\n",
        "print(\"Tensorflow version: \" + tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is used.\n",
            "Tensorflow version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJpqmZYFUEWx"
      },
      "source": [
        "Now, please run the following cell to download the dataset that you will use to build your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7UHmFBevLa3"
      },
      "source": [
        "**The following code section is from the template provided by the AI Challenge organiser.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KID_KMfXQGHw",
        "outputId": "3c7ded30-9bb9-4332-b3b6-a5ca57c71f58"
      },
      "source": [
        "# Download dataset:\n",
        "!wget http://206.12.93.90:8080/simpson_dataset/simpsons_train.tar.gz \n",
        "# Unzip the dataset:\n",
        "!tar -xzvf simpsons_train.tar.gz > /dev/null\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-13 22:28:38--  http://206.12.93.90:8080/simpson_dataset/simpsons_train.tar.gz\n",
            "Connecting to 206.12.93.90:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 488194922 (466M) [application/x-gzip]\n",
            "Saving to: ‘simpsons_train.tar.gz’\n",
            "\n",
            "simpsons_train.tar. 100%[===================>] 465.58M  19.4MB/s    in 25s     \n",
            "\n",
            "2021-05-13 22:29:03 (18.6 MB/s) - ‘simpsons_train.tar.gz’ saved [488194922/488194922]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzGsHSkTUEWx"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n",
        "We use the Simpson character data available in [kaggle](https://www.kaggle.com/alexattia/the-simpsons-characters-dataset). \n",
        "\n",
        "This dataset is composed of 20 folders (one for each character) with 400-2000 images in each folder. The total number of images is 19,548.\n",
        "\n",
        "For reading these images, we use `DirectoryIterator` in `tf.keras.preprocessing.image` that is an iterator capable of reading images from a directory on disk and is capable to extract labels. We also use `ImageDataGenerator` to split this dataset into training and validation set, this later is used to tune the hyperparameters of our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-84wHvvD-Ot"
      },
      "source": [
        "**The following code section is from the template provided by the AI Challenge organiser.**\n",
        "\n",
        "**Changes were made to:**\n",
        "\n",
        "**- image size**\n",
        "\n",
        "**- batch size**\n",
        "\n",
        "**An imageDataGenerator was added to feed augmented images into the model for data augmentation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpgROocq9k8k",
        "outputId": "d5dfbdda-679e-4f83-ad8b-63efa84ab8dc"
      },
      "source": [
        "'''\n",
        "    Split train and validation.\n",
        "'''\n",
        "# We define the size of input images to 128x128 pixels.\n",
        "image_size = (224, 224)\n",
        "# We define the batch size\n",
        "batch_size = 8\n",
        "# Create an image generator with a fraction of images reserved for validation:\n",
        "image_generator = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1, \n",
        "        zoom_range=0.2,\n",
        "        validation_split=0.1)\n",
        "\n",
        "val_image_generator = ImageDataGenerator(validation_split=0.1)\n",
        "# Now, we create a training data iterator by creating batchs of images of the same size as \n",
        "# defined previously, i.e., each image is resized in a 64x64 pixels format.\n",
        "train_ds =  DirectoryIterator(\n",
        "    \"dataset/simpsons_train/\",\n",
        "    image_generator,\n",
        "    class_mode='categorical',\n",
        "    seed=1337,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    subset = 'training',\n",
        ")\n",
        "\n",
        "# Similarly, we create a validation data iterator by creating batchs of images of the same size as \n",
        "# defined previously, i.e., each image is resized in a 64x64 pixels format.\n",
        "val_ds = DirectoryIterator(\n",
        "    \"dataset/simpsons_train/\",\n",
        "    val_image_generator,\n",
        "    class_mode='categorical',\n",
        "    seed=1337,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    subset = 'validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# We save the list of classes (labels).\n",
        "class_names = list(train_ds.class_indices.keys())\n",
        "\n",
        "# We also save the number of labels.\n",
        "num_classes = train_ds.num_classes\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17603 images belonging to 20 classes.\n",
            "Found 1945 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTuW6rRvL8L3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3KeymU-UEWy"
      },
      "source": [
        "# Exploring the data\n",
        "\n",
        "Now, we do data exploration to show you samples of the images and their labels and some statistics to help you in understanding the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKsqI7soUEWy"
      },
      "source": [
        "# Designing the model\n",
        "\n",
        "\n",
        "We now design the architecture for the task. The artchitecture below consists of:\n",
        "1. `Rescaling layer:` whose role is to normalize the input data to values between 0 and 1. This will help in speed up the training process.\n",
        "2. `Flatten layer:` whose role is to flatten the 3D volume.\n",
        "3. `Dense layers`: one dense layer followed by a classification layer with a softmax activation function.\n",
        "\n",
        "Please note that you will have to design your own model if you want to beat the baseline and be at the top of the leaderboard!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMf3WuqQuDb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aaa5057-88bc-4e3f-8b47-d458002ae58f"
      },
      "source": [
        "# CONVOLUTIONAL PRETRAINED BASE\n",
        "\n",
        "conv_base = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEp_xmaTd1gQ",
        "outputId": "f1ef354f-1ccf-449d-f77c-73cec13246d5"
      },
      "source": [
        "# Defining your model here:\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Activation\n",
        "\n",
        "inputs = keras.Input(shape=image_size + (3,))\n",
        "x = inputs\n",
        "x - layers.experimental.preprocessing.Rescaling(1./255)(x)\n",
        "\n",
        "\n",
        "x = conv_base(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "#x = layers.Flatten()(x)\n",
        "#x = layers.Dropout(0.7)(x)\n",
        "\n",
        "\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.7)(x)\n",
        "x = Dense(20)(x)\n",
        "outputs = Activation(activation='softmax')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.00001),\n",
        "              loss='CategoricalCrossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "densenet201 (Model)          multiple                  18321984  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              1967104   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                20500     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 20,309,588\n",
            "Trainable params: 20,080,532\n",
            "Non-trainable params: 229,056\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1uScTSLgfZe"
      },
      "source": [
        "conv_base.trainable = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzuQ5JNswKwK",
        "outputId": "1fb8e9a3-cdf3-4c04-90d0-a22c046e60f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiD2-Jro0ipG"
      },
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(lr=0.00001),\n",
        "              loss='CategoricalCrossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp2RpsdogUqI",
        "outputId": "2267368a-2eca-4790-a101-268e51eee880"
      },
      "source": [
        "# Create callback to save best model\n",
        "model_name=\"densenet201FMaug3\"\n",
        "model_save_name = \"model_best_accuracy_\"+model_name\n",
        "\n",
        "highest_acc = 0\n",
        "class high_acc_Callback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "          current_acc=logs.get('val_accuracy')\n",
        "          global highest_acc\n",
        "          global class_names\n",
        "          global image_size\n",
        "          #print (\"image_size: \",image_size)\n",
        "          from google.colab import drive\n",
        "          if current_acc > highest_acc:\n",
        "            print(\"Highest accuracy so far: \",current_acc)\n",
        "            highest_acc=current_acc\n",
        "            #strip activation layer\n",
        "            with h5py.File('/content/gdrive/My Drive/'+\"dnsnetaugSubt_\"+\"{:.5f}\".format(current_acc)+ '_model.h5', mode='w') as f:\n",
        "              hdf5_format.save_model_to_hdf5(model, f)\n",
        "              f.attrs['class_names'] = class_names\n",
        "              f.attrs['image_size'] = image_size\n",
        "          \n",
        "callbacks = high_acc_Callback()\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  epochs=50,\n",
        "  validation_data=val_ds,\n",
        "  callbacks=[callbacks]\n",
        "  )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 1.8305 - accuracy: 0.4937Highest accuracy so far:  0.922879159450531\n",
            "2201/2201 [==============================] - 327s 149ms/step - loss: 1.8305 - accuracy: 0.4937 - val_loss: 0.3221 - val_accuracy: 0.9229\n",
            "Epoch 2/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.8976Highest accuracy so far:  0.974293053150177\n",
            "2201/2201 [==============================] - 323s 147ms/step - loss: 0.4009 - accuracy: 0.8976 - val_loss: 0.1038 - val_accuracy: 0.9743\n",
            "Epoch 3/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9492Highest accuracy so far:  0.982519268989563\n",
            "2201/2201 [==============================] - 321s 146ms/step - loss: 0.1984 - accuracy: 0.9492 - val_loss: 0.0845 - val_accuracy: 0.9825\n",
            "Epoch 4/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9660Highest accuracy so far:  0.9866324067115784\n",
            "2201/2201 [==============================] - 319s 145ms/step - loss: 0.1326 - accuracy: 0.9660 - val_loss: 0.0745 - val_accuracy: 0.9866\n",
            "Epoch 5/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9764Highest accuracy so far:  0.9871465563774109\n",
            "2201/2201 [==============================] - 324s 147ms/step - loss: 0.0945 - accuracy: 0.9764 - val_loss: 0.0777 - val_accuracy: 0.9871\n",
            "Epoch 6/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9809Highest accuracy so far:  0.9897172451019287\n",
            "2201/2201 [==============================] - 322s 146ms/step - loss: 0.0735 - accuracy: 0.9809 - val_loss: 0.0781 - val_accuracy: 0.9897\n",
            "Epoch 7/50\n",
            "2201/2201 [==============================] - 319s 145ms/step - loss: 0.0604 - accuracy: 0.9848 - val_loss: 0.0762 - val_accuracy: 0.9877\n",
            "Epoch 8/50\n",
            "2201/2201 [==============================] - 316s 143ms/step - loss: 0.0483 - accuracy: 0.9886 - val_loss: 0.0873 - val_accuracy: 0.9892\n",
            "Epoch 9/50\n",
            "2201/2201 [==============================] - 315s 143ms/step - loss: 0.0378 - accuracy: 0.9906 - val_loss: 0.0761 - val_accuracy: 0.9897\n",
            "Epoch 10/50\n",
            "2201/2201 [==============================] - 319s 145ms/step - loss: 0.0341 - accuracy: 0.9918 - val_loss: 0.0804 - val_accuracy: 0.9892\n",
            "Epoch 11/50\n",
            "2201/2201 [==============================] - 314s 142ms/step - loss: 0.0255 - accuracy: 0.9934 - val_loss: 0.0890 - val_accuracy: 0.9887\n",
            "Epoch 12/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0257 - accuracy: 0.9932 - val_loss: 0.0882 - val_accuracy: 0.9897\n",
            "Epoch 13/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0223 - accuracy: 0.9938 - val_loss: 0.0946 - val_accuracy: 0.9897\n",
            "Epoch 14/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9955Highest accuracy so far:  0.9902313351631165\n",
            "2201/2201 [==============================] - 316s 143ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.1064 - val_accuracy: 0.9902\n",
            "Epoch 15/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9965Highest accuracy so far:  0.9912596344947815\n",
            "2201/2201 [==============================] - 316s 144ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.0948 - val_accuracy: 0.9913\n",
            "Epoch 16/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.0984 - val_accuracy: 0.9897\n",
            "Epoch 17/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.1043 - val_accuracy: 0.9913\n",
            "Epoch 18/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0124 - accuracy: 0.9968 - val_loss: 0.1122 - val_accuracy: 0.9902\n",
            "Epoch 19/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.1162 - val_accuracy: 0.9907\n",
            "Epoch 20/50\n",
            "2201/2201 [==============================] - 315s 143ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 0.1068 - val_accuracy: 0.9897\n",
            "Epoch 21/50\n",
            "2201/2201 [==============================] - 316s 144ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.1242 - val_accuracy: 0.9907\n",
            "Epoch 22/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.1189 - val_accuracy: 0.9902\n",
            "Epoch 23/50\n",
            "2201/2201 [==============================] - 314s 142ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.1317 - val_accuracy: 0.9913\n",
            "Epoch 24/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.1221 - val_accuracy: 0.9907\n",
            "Epoch 25/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.1269 - val_accuracy: 0.9902\n",
            "Epoch 26/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.1327 - val_accuracy: 0.9892\n",
            "Epoch 27/50\n",
            "2201/2201 [==============================] - 311s 141ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 0.1332 - val_accuracy: 0.9902\n",
            "Epoch 28/50\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9986Highest accuracy so far:  0.992802083492279\n",
            "2201/2201 [==============================] - 320s 145ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.1321 - val_accuracy: 0.9928\n",
            "Epoch 29/50\n",
            "2201/2201 [==============================] - 316s 143ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.1248 - val_accuracy: 0.9918\n",
            "Epoch 30/50\n",
            "2201/2201 [==============================] - 316s 144ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 0.1367 - val_accuracy: 0.9907\n",
            "Epoch 31/50\n",
            "2201/2201 [==============================] - 316s 143ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.1450 - val_accuracy: 0.9918\n",
            "Epoch 32/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.1546 - val_accuracy: 0.9913\n",
            "Epoch 33/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.1410 - val_accuracy: 0.9907\n",
            "Epoch 34/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.1394 - val_accuracy: 0.9913\n",
            "Epoch 35/50\n",
            "2201/2201 [==============================] - 311s 141ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1280 - val_accuracy: 0.9913\n",
            "Epoch 36/50\n",
            "2201/2201 [==============================] - 314s 143ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1363 - val_accuracy: 0.9913\n",
            "Epoch 37/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1348 - val_accuracy: 0.9902\n",
            "Epoch 38/50\n",
            "2201/2201 [==============================] - 314s 143ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.1389 - val_accuracy: 0.9907\n",
            "Epoch 39/50\n",
            "2201/2201 [==============================] - 314s 143ms/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.1482 - val_accuracy: 0.9913\n",
            "Epoch 40/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.1518 - val_accuracy: 0.9902\n",
            "Epoch 41/50\n",
            "2201/2201 [==============================] - 315s 143ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.1514 - val_accuracy: 0.9897\n",
            "Epoch 42/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.1341 - val_accuracy: 0.9897\n",
            "Epoch 43/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.1572 - val_accuracy: 0.9887\n",
            "Epoch 44/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.1522 - val_accuracy: 0.9902\n",
            "Epoch 45/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.1494 - val_accuracy: 0.9902\n",
            "Epoch 46/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.1349 - val_accuracy: 0.9913\n",
            "Epoch 47/50\n",
            "2201/2201 [==============================] - 314s 143ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.1462 - val_accuracy: 0.9892\n",
            "Epoch 48/50\n",
            "2201/2201 [==============================] - 312s 142ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.1399 - val_accuracy: 0.9907\n",
            "Epoch 49/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1492 - val_accuracy: 0.9907\n",
            "Epoch 50/50\n",
            "2201/2201 [==============================] - 313s 142ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.1534 - val_accuracy: 0.9918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk3hKnvxUEW3"
      },
      "source": [
        "# Acknowledgment\n",
        "\n",
        "\n",
        "**Author:** [Mohamed Reda Bouadjenek](https://rbouadjenek.github.io/), Lecturer of Applied Artificial Intelligence, \n",
        "\n",
        "**Institution:** Deakin University, School of Information Technology, Faculty of Sci Eng & Built Env\n",
        "\n",
        "**Adress:** Locked Bag 20000, Geelong, VIC 3220\n",
        "\n",
        "**Phone:** +61 3 522 78380\n",
        "\n",
        "**Email:** reda.bouadjenek@deakin.edu.au\n",
        "\n",
        "**www.deakin.edu.au**\n",
        "\n",
        "<div>\n",
        "<img style=\"float: left;\" src=\"https://github.com/rbouadjenek/deakin-simpsons-challenge2020/blob/main/images/deakin2.png?raw=1\" width=\"200\" >\n",
        "</div>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<div>  <a href=\"https://twitter.com/DeakinAI2021\" > <img style=\"float: left;\" src=\"https://irisconnect.com/uk/wp-content/uploads/sites/3/2020/12/twitter-Follow-us-button.png\" width=\"200\" > </a>\n",
        "</div>"
      ]
    }
  ]
}